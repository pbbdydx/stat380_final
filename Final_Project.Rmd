---
title: "Final_Project"
author: Dylan Bauer, Prajwal Bhandari, Cameron Moore
output: html_document
---

## Front Matter

```{r}
# clear env, load libraries and data
rm(list = ls())
library(tidyverse)
library(stringdist)
library(caret)
library(randomForest)
library(e1071)
library(emmeans)

cod_p1 <- read_csv('CODGames_p1_380.csv')
cod_p2 <- read_csv('CODGames_p2_380.csv')
gm_data <- read_csv('CODGameModes.csv')
map_data  <- read.csv('CODMaps.csv')

# we combine the results of player 1 and player 2 according to the instructions
# so first add a column to indicate which player it is in case it is used for the future problems
cod_p1$player = 1
cod_p2$player = 2

full_data <- rbind(cod_p1, cod_p2)

# lowercase column names.
# AI disclosure: We asked chatgpt 4o to modify the column names with regex
names(full_data) <- tolower(gsub("([a-z])([A-Z])", "\\1_\\2", names(full_data)))

```
### Preliminary Data Cleaning
Before we start working on any of the tasks, we should clean the full dataset thoroughly so we do not run into any issues later on.

We clean the map names for the `map1`, `map2`, `choice` columns, edit any bad vote records in the `map_vote` column, and change the `game_type` column to follow the same pattern. We use the `setdiff()` function to compare the current map names and the clean map names, which is why we have the specific cases in our `case_when` clauses. Finally, we separate the `map_vote` column so we can do task 1 effectively. Dropping columns with too many missing values may also be useful but we keep them for now.

```{r}
# get a list of the clean names
clean_maps <- as.vector(unique(map_data$Name))

clean_data <- full_data %>%
  mutate(
    map1 = case_when(
      map1 %in% clean_maps ~ map1,
      map1 %in% c('Collateral', 'Collateral Streik', 'Collaterol Strike') ~ 'Collateral Strike',
      map1 == 'Deprogam' ~ 'Deprogram',
      map1 == 'Miami Stirke' ~ 'Miami Strike',
      map1 == 'Ruah' ~ 'Rush',
      map1 == 'Riad' ~ 'Raid',
      map1 == 'Drive-in' ~ 'Drive-In'
      ),
    map2 = case_when(
      map2 %in% clean_maps ~ map2,
      map2 %in% c('Miami Stirke', 'Miami Sstrike') ~ 'Miami Strike',
      map2 == 'yamantau' ~ 'Yamantau', 
      map2 == 'Collateral' ~ 'Collateral Strike',
      map2 == 'Amrada Strike' ~ 'Armada Strike',
      map2 == 'Drive-in' ~ 'Drive-In'
      ),
    choice = case_when(
      choice %in% clean_maps ~ choice,
      choice %in% c("APocalypse","Apocolypse") ~ 'Apocalypse',
      choice %in% c('Collateral', 'Collaterel Strike') ~ 'Collateral Strike',
      choice == 'Deisel' ~ 'Diesel',
      choice == 'Drive-in' ~ 'Drive-In',
      choice == 'Riad' ~ 'Raid'
    ),
    # notice there are 'X o Y' values for the map vote so use gsub to swap them
    map_vote = gsub(' o ', ' to ', map_vote),
    # clean the game type variable since it will be used in task 3
    game_type = case_when(
      game_type == 'HC - TDM' ~ 'TDM',
      game_type == 'HC - Kill Confirmed' ~ 'Kill Confirmed',
      game_type == 'HC - Hardpoint' ~ 'Hardpoint',
      game_type == 'HC - Domination' ~ 'Domination'
    )
  ) %>%
  # separate map vote column. The larget number is always written first 
  separate(map_vote, into = c('winning_map_vote', 'losing_map_vote'), sep = ' to ', extra = 'merge', convert = TRUE)

# make sure the name of every map is cleaned for all 3 map columns. (set diff is NA only for missing maps)
setdiff(clean_data$map1, map_data$Name)
setdiff(clean_data$map2, map_data$Name)
setdiff(clean_data$choice, map_data$Name)

```

## Task 1

Research Question: Which maps are the most likely to win the map vote when they are an option?

To answer this question, we will need the 5 variables `map1`, `map2`, `winning_map_vote`, `losing_map_vote` and `choice` . For each map, we want to record the number of times it was entered into the vote and then calculate the number of times it was chosen for `choice` to calculate the probability. 

Then, after calculating the probabilities of each map, we can make a ggplot visualization to display the results.

```{r}
map_vote_data <- clean_data %>%
  select(map1, map2, winning_map_vote, losing_map_vote, choice)
```

The dataset looks good by inspection, except for one strange value in the 489th observation where both `map1` and `map2` have the same number of votes, but `choice` is set to `map2`. We can fix this by changing the map choice to be the first map. Now we are ready to answer the research question.

```{r}
map_vote_data[489,]$choice <- map_vote_data[489,]$map1
```

We run into another issue of only 1 of either `map1` or `map2` having data while the other is missing. For some of the cases, we can remedy this by checking against the `choice` column. We do this for both the `map1` and `map2` columns for `map_vote_data`. 

```{r}
# we filter here to show the few cases where this happens. in the next code chunk,
# we clean the data to a consistent format.
map_vote_data %>%
  filter(
    is.na(map1) & !is.na(map2) | is.na(map2) & !is.na(map1)
  )
```

There are a few cases that we work through to fix this issue. If one column is the same as the choice and the other column is missing, there isnt much that can be done. If the choice is unknown, then we also cannot do much. Finally, if one of the columns exist and is not the same as choice, then the other column has to be the choice. 

```{r}
map_vote_data <- map_vote_data %>%
  mutate(
    # since we only have a few specific cases, we can 'hard-code' them into 
    # the data wrangling
    map1 = if_else(map2 != choice, choice, map1),
    map2 = if_else(map1 != choice, choice, map2),
    choice = if_else(map2 == choice & is.na(map1), NA, choice)
  )
```


To answer this question an ideal dataset would have the following columns: `map`, `win`, `method`. We create two data frames that contain map and winning statistics for the two maps: the first one using only the `map1` column as the map and the second one only using the `map2` column as the map. We call them `m1_stats` and `m2_stats` respectively and concatenate the data frames. Then we have all voting results of the two maps for all the games in a tidy format. Finally, we can use grouping to calculate the statistics for a given map. 

```{r}
# create the dataframes that are used for this question
m1_stats <- map_vote_data %>%
  select(-map2) %>%
  subset(!is.na(map1)) %>%
  group_by(map1) %>%
  reframe(
    win = (map1 == choice),
    # there are 3 cases for the method: either a map lost, won by votes, or got forced to win because it was map1
    method = case_when(
    map1 != choice ~ 'Did Not Win',
    map1 == choice & winning_map_vote > losing_map_vote ~ 'Voted',
    map1 == choice & winning_map_vote == losing_map_vote ~ 'Forced'
    )
  ) %>%
  rename(
    map = map1
  )

m2_stats <- map_vote_data %>%
  select(-map1) %>%
  subset(!is.na(map2)) %>%
  group_by(map2) %>%
  reframe(
    win = map2 == choice,
    # there are 2 cases for map2: either it lost or it won by votes. it is never forced to be the map.
    method = case_when(
    map2 == choice & winning_map_vote > losing_map_vote ~ 'Voted',
    !(map2 == choice & winning_map_vote > losing_map_vote) ~ 'Did Not Win' # if it was not voted, then it did not win
    )
  ) %>%
  rename(
    map = map2
  )

map_stats <- rbind(m1_stats, m2_stats)

# make sure map_stats does not have any NA values

sum(is.na(map_stats))
```

Finally, we use our new data frame `map_stats` to answer the question and make the visualization. This data frame contains only matches in which all map related columns are not missing. If we want to answer what maps are the most likely to win a vote when they are chosen, we should get the probabilities that they win given that they are chosen and sort them by map. What we can do is get the probability of win for a single game by doing win/times the map was chosen which will calculate the probability of winning that single instance. Then we can ungroup the data before grouping by both map and the method of winning to get probabilities for each combination of map/method. 

```{r}
map_probs <- map_stats %>%
  group_by(map) %>%
  mutate(
    times_chosen = n(),
    prob_win = win/times_chosen
  ) %>% 
  ungroup() %>%
  group_by(map, method) %>%
  summarize(
    total_prob = sum(prob_win),
    .groups = 'drop'
  ) %>% 
  group_by(map) %>%
  mutate(
    total_sum = sum(total_prob)
    ) %>%
  ungroup() %>%
  mutate(map = reorder(map, total_sum))
map_probs
```

We can interpret the table above like so: When a map, say 'Amerika', was up for vote, it was chosen to be the game map 48% of the time, forced to be the map 3% of the time, and lost the other 49%, although the loss percentage is not explicitly reported in the table. 

```{r}
# making the plot
map_probs %>%
  filter(total_prob != 0) %>%
  ggplot(aes(x = map, y = total_prob, fill = method)) + 
  geom_bar(stat = 'identity', color = 'black') + 
  labs(
    x = 'Map', 
    y = 'Total Probability',
    fill = 'Method of Win',
    title = 'Total Probability of a Map To Be Chosen When it Appears For the Map Vote'
    ) + 
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

From the visualization and summary statistics above, we see that Nuketown '84, Crossroads Strike, and Raid are the three maps with the highest probabilities of being chosen for a vote with p = 0.820, 0.775, and 0.772 respectively. What is surprising is that Raid has the highest probability of winning via voting at p = 0.727, while the two maps with higher total probabilities  make up for lower voting rates with higher rated of being forced to win. It's interesting to see that Diesel has the smallest probability that it is forced to be the map at 0.018, but is actually the one of the highest maps for probability of being voted as the game map with p = 0.690, falling closely in 4th place behind Nuketown '84 with p = 0.692, and Crossroads Strike with p = 0.706. On the other side of the spectrum, Miami, Echelon, and Deprogram / Armada Strike (tied) are chosen the least at p = 0.120, 0.178, and 0.244.



## Task 2 - Repeat Task 1 using Generative AI.

We will be using ChatGPT on version GPT-4o. The prompt we used was the first copying the first question into the AI. Then it did not understand what data files had which data in it so we had to inform it of what each data file had in it (CodMaps, CodModes, etc.). We used the given project data set description from the final project file. There were some errors. First, some of the packages it included we had to download and use, which caused an error when we didn't have it. We asked about the error and it gave us the package we needed. The final follow up we asked was an error in the graph. There was a bottom bar that was 1.00 or 100% with no name so we asked why is that there. It gave us a command to fix that. 

```{r}
games <- bind_rows(cod_p1, cod_p2)

# --- 2. Standardize and Clean Map Names ---
# Clean function: trim whitespace and correct spelling using string distance
clean_map <- function(name, reference_names) {
  name <- str_trim(name)
  if (is.na(name)) return(NA)
  match_index <- amatch(name, reference_names, maxDist = 2)
  if (!is.na(match_index)) return(reference_names[match_index])
  return(name)
}

# List of valid map names from CODMaps.csv
valid_maps <- unique(str_trim(map_data$Name))

# Clean all relevant map columns
games <- games %>%
  mutate(
    Map1 = sapply(Map1, clean_map, reference_names = valid_maps),
    Map2 = sapply(Map2, clean_map, reference_names = valid_maps),
    Choice = sapply(Choice, clean_map, reference_names = valid_maps),
    MapVote = sapply(MapVote, clean_map, reference_names = valid_maps)
  )

# --- 3. Filter to Only Games with Voting Info ---
# Keep rows where Map1 and Map2 are both non-missing (indicates vote occurred)
games_votes <- games %>%
  filter(!is.na(Map1) & !is.na(Map2))

# --- 4. Determine the Winner of Each Vote ---
# Count number of votes for Map1 and Map2
vote_counts <- games_votes %>%
  rowwise() %>%
  mutate(
    Map1_votes = sum(MapVote == Map1, na.rm = TRUE),
    Map2_votes = sum(MapVote == Map2, na.rm = TRUE),
    Winner = case_when(
      Map1_votes > Map2_votes ~ Map1,
      Map2_votes > Map1_votes ~ Map2,
      Map1_votes == Map2_votes ~ Map1  # tie breaker: Map1 wins
    )
  ) %>%
  ungroup()

# --- 5. Reshape for Aggregation ---
# Long format: 1 row per map per match
long_maps <- vote_counts %>%
  pivot_longer(cols = c(Map1, Map2), names_to = "MapSlot", values_to = "Map") %>%
  mutate(
    Won = Map == Winner
  )

# --- 6. Aggregate Statistics ---
# After cleaning and reshaping
long_maps <- long_maps %>%
  filter(!is.na(Map) & Map != "")


map_stats <- long_maps %>%
  group_by(Map) %>%
  summarise(
    Times_Offered = n(),
    Times_Won = sum(Won),
    Win_Rate = Times_Won / Times_Offered
  ) %>%
  arrange(desc(Win_Rate))

# --- 7. Visualization ---
ggplot(map_stats, aes(x = reorder(Map, -Win_Rate), y = Win_Rate)) +
  geom_col(fill = "#2a9df4") +
  coord_flip() +
  labs(
    title = "Map Win Rates When Offered as a Voting Option",
    x = "Map",
    y = "Win Rate"
  ) +
  theme_bw()
```

Comparing solutions

Point 1:

To start, the data cleaning approach was different. Our solution takes a more manual approach while the AI uses more automated function. It uses the function `amatch` from a new package to replace certain misspelled words. We used `setdiff()` and `case_when` for replacing misspelled words that we knew where misspelled. The manual approach has some benefits like controlling what we change and why we changed it compared to AI where you don't really know what happened. AI used a more automated approach using `amatch`. This is probably better for a large list of typos that may be unknown. This could cause problems because it could make false corrections and be hard to debug. In conclusion, the manual cleaning we did was the safer option compared to AI's option.

Point 2:

Next, the handling of missing information was different between the two. In our solution, we had a little more attention to detail. For example, we fixed cases where both map1 and map2 were not present. We corrected ties when the wrong map was chosen. We fixed the `choice` column for missing values as well. This made sure the win rates did not have any incomplete or missing data values. On the other hand, the AI handled the ties, but did not fix the one map entries or missing values. This caused some noticeable discrepancies with our final results, where the top maps and their selection rates do not match with the ones in our solutions. In all, we had thoughtful corrections that made the analysis more robust while ChatGPT opted for a more algorithmic approach that may not have done the due diligence that was warranted in answering research question 1.

Point 3:

Finally, the presentation and results were different. We both were trying to solve the same thing but got different results. We used a stacked bar chart that included forced wins as part of the total. The AI had a horizontal chart and only used the voting wins. It did not account for forced wins like we did. For our method, we counted each map in every vote (map1 and map2) and summed a win or loss. We combined them and calculated the probability. On the other hand, AI used a single row and only counted how many times a map was selected out of the times it was available. To sum, our method has a deeper understanding of the data while AI is simpler and doesn't go as in depth. 

Using artificial intelligence tools for statistical analysis is a double edged sword we hope to have demonstrated the increased risk of side effects when there are edge cases present in the data and why these tasks should be done by trained analysts and statisticians whenever possible.


## Task 3

Research Question -  How does the game type affect TotalXP after accounting for the Score?

First, some of the data cleaning was included in the beginning of our document. But we did notice a lot of cases where game type and/or score was a NA value so we got rid of those. 

```{r}
sum(is.na(clean_data$game_type))
sum(is.na(clean_data$score))

no_na_clean <- 
  clean_data %>%
  filter(!is.na(game_type)) %>%
  filter(!is.na(score))
```

There were 364 NA values in the game_type column and 31 score values that were NA. There could have been some overlapping also. Because dealing with missing values presents itself to greater challenges than anticipated, we opt to only work with the data that we do have and thus filter for rows where the data does exist.

Next we need to do some EDA and look at relationships between the variables. We will make a summary table and a few different visualizations.

```{r}
no_na_clean %>%
  group_by(game_type) %>%
  summarize(
    N = n(),
    mean_Score = mean(score),
    median_Score = median(score),
    mean_TotalXP = mean(total_xp),
    median_TotalXP = median(total_xp)
  )
```

Look at the table above, we can see that the game type played the most was TDM at 346 and least was Domination at 11. Domination has the highest mean score at 3174.091 and also mean total xp at 16151.091. The lowest mean score is Kill Confirmed with 1983.571 and also the lowest mean Xp at 9089.571. 

```{r}
# Distribution of GameType
ggplot(mapping = aes(x = game_type), data = no_na_clean) +
  geom_bar(fill = 'steelblue', color = 'black') +
  labs(title = "Distribution of Game Types", 
       x = "Game Type",
       y = "Count") +
  theme_bw()
```

We can see the distribution of each game type above that goes with our table. TDM has far more times played then the rest of the game types. 

```{r}
ggplot(mapping = aes(x = score, y = total_xp, color = game_type), data = no_na_clean) +
  geom_point() + 
  geom_smooth(method = lm, se = F) +
  facet_wrap(~ game_type) +
  labs(title = "TotalXP vs Score by Game Type", 
       x = "Score", 
       y = "Total Xp",
       color = "Game Type") +
  theme_bw()
```

Looking at the plots here, we can see a positive relationship between score and total xp for all the game types. As the score increases, the total xp will also increase. For some game types, the slope is steeper meaning there might be a evidence that some game modes give more xp per score compared to others. 


```{r}
ggplot(mapping = aes(x = game_type, y = total_xp), data = no_na_clean) +
  geom_boxplot(fill = 'steelblue', color = 'black') +
  labs(title = "TotalXP Distribution by GameType",
       x = "Game Type",
       y = "Total Xp"
       ) + 
  theme_bw()
```

Looking at the boxplots here, we can see a lot of high outliers for Hardpoint and TDM. The highest median XP is Domination and lowest is Kill Confirmed.


```{r}
model3 <- lm(total_xp ~ score + game_type, data = no_na_clean)
summary(model3)
```

In the above model we fitted the linear regression model with the response Total Xp and the input variables as Score and Game Type. For game type, the baseline is domination. Score is a significant predictor of total XP. If we hold game type constant, a 1-unit increase in score will lead to an increase in total xp by 2.36. Now to answer the question, we will have to compare each game type to the baseline domination. If we keep score constant, the game type "Hardpoint" will have on average 208 less XP than domination. When accounting for score, the game type "Kill Confirmed" will have 4,251 fewer xp on average than domination. Finally, when accounting for score, if the game type is "TDM", it is 2,892 less XP than domination on average. "Domination"'s intercept of 8,658.8870 isn't realistic because you would need a score of zero. In all, after accounting for score, game type has a moderate affect on TotalXP. When playing in Kill Confirmed and TDM, the players were getting a more notable decrease in XP compared to domination. When playing Hardpoint, the difference from Domination was not that significantly different when keeping score constant. 

Finally, to conduct a more robust analysis on the effect of the game type and score on total XP, we will run an ANOVA model on `model3` and then post-hoc diagnostics if results are significant.
```{r}
aov_model <- aov(model3)
summary(aov_model)
```

The ANOVA model agrees with our previous assertion that score is a significant predictor and also shows that game_type is a significant predictor. Now, we conduct a post-hoc comparison of means using Bonferroni's adjustment to account for family-wise error rates.

```{r}
em_aovmodel <- emmeans(aov_model, ~ game_type)
pairs(em_aovmodel, adjust = 'bonferroni')
```

Using this post-hoc analysis, only one game type comparison yielded a significant result: Hardpoint vs TDM. 

## Task 4

Research Question:
Can we predict whether Player 1 will achieve an above-median XP total in a Call of Duty match based on in-game performance metrics like eliminations, damage dealt, and score? By framing this as a binary classification problem, we sought to evaluate how well different statistical learning methods could distinguish between high- and low-performing matches.

```{r}
# Filter to Player 1 data and ensure relevant columns are clean
xp_class_data <- clean_data %>%
  filter(player == 1) %>%
  select(eliminations, score, damage, total_xp) %>%
  drop_na()

# Create a binary target: 1 if above median XP, 0 otherwise
median_xp <- median(xp_class_data$total_xp)
xp_class_data <- xp_class_data %>%
  mutate(high_xp = ifelse(total_xp > median_xp, 1, 0)) %>%
  select(-total_xp)

# Stratified split using caret
library(caret)
set.seed(42)
train_index <- createDataPartition(xp_class_data$high_xp, p = 0.8, list = FALSE)
train_set <- xp_class_data[train_index, ]
test_set <- xp_class_data[-train_index, ]

# --- Logistic Regression ---
log_model <- glm(high_xp ~ ., data = train_set, family = "binomial")
log_preds <- predict(log_model, test_set, type = "response")
log_class <- ifelse(log_preds > 0.5, 1, 0)
log_acc <- mean(log_class == test_set$high_xp)

# --- Random Forest ---
library(randomForest)
rf_model <- randomForest(as.factor(high_xp) ~ ., data = train_set)
rf_preds <- predict(rf_model, test_set)
rf_acc <- mean(rf_preds == test_set$high_xp)

# --- SVM ---
library(e1071)
svm_model <- svm(as.factor(high_xp) ~ ., data = train_set, kernel = "radial")
svm_preds <- predict(svm_model, test_set)
svm_acc <- mean(svm_preds == test_set$high_xp)

# Compare results
model_results <- tibble(
  Model = c("Logistic Regression", "Random Forest", "SVM"),
  Accuracy = c(log_acc, rf_acc, svm_acc)
)
model_results
```

To begin, we filtered the dataset to include only rows corresponding to Player 1. We selected three predictors-eliminations, score, and damage—and the outcome variable, total XP. We then created a new binary target variable named high_xp, which was assigned a value of 1 if the player’s XP for that match exceeded the overall median, and 0 otherwise. This ensured a balanced classification task. After removing any rows with missing values, we used a stratified 80/20 train-test split to prepare the data for modeling.

Three classification methods were implemented: logistic regression, random forest, and support vector machine (SVM). Logistic regression models the log-odds of the response as a linear combination of predictors and is useful for understanding the direction and strength of each variable's influence. Random forest, an ensemble method, constructs many decision trees and combines their results to make robust, low-variance predictions. Finally, SVM, which was not covered in class, was chosen as the new method to explore. SVM finds the hyperplane that best separates the classes in a high-dimensional space and can model non-linear relationships using kernel functions. In our case, we used the radial basis function (RBF) kernel.

All three models were trained on the same training data and evaluated using classification accuracy on the test set. Logistic regression achieved an accuracy of about 70.8%, random forest improved slightly with 72.6%, and SVM outperformed both with an accuracy of 75.2%. These results suggest that non-linear classification methods such as SVM and random forest are better suited for this task, likely because the relationship between the predictors and XP is not strictly linear.
In conclusion, we were able to build effective classification models to predict whether Player 1 would gain above-median XP in a match. Among the three methods, SVM provided the best results, indicating its strong performance in handling complex patterns in the data. These insights could help players or analysts understand what types of in-game behavior are most associated with higher XP outcomes.
